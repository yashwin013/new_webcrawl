"""
Processor Worker

Simplified router that sends all crawled pages to PDF processor.

NEW ARCHITECTURE (Docling-Only):
- All crawled pages have PDFs (generated by Playwright)
- Processor worker is now just a router to PDF queue
- All processing happens in PDF worker (Docling + HybridChunker)
- No more DOM text extraction or sentence-based chunking
"""

import asyncio
from pathlib import Path
from typing import Optional

from app.config import get_logger
from app.orchestrator.workers.base import BaseWorker
from app.orchestrator.queues import QueueManager
from app.orchestrator.models.task import ProcessTask, PdfTask
from app.crawling.models.document import Page

logger = get_logger(__name__)


class ProcessorWorker(BaseWorker):
    """
    Processor worker - routes all pages to PDF processor for Docling.
    
    NEW SIMPLIFIED ARCHITECTURE:
    - All crawled pages have PDFs (generated by Playwright)
    - All PDFs go through Docling for structure-aware processing
    - Uses HybridChunker for consistent, high-quality chunking
    
    Responsibilities (CPU - now just routing):
    - Pull ProcessTask from processing_queue
    - Route ALL pages to pdf_queue for Docling processing
    - No more DOM text extraction or sentence-based chunking
    """
    
    def __init__(
        self,
        worker_id: str,
        queue_manager: QueueManager,
    ):
        super().__init__(worker_id, queue_manager)
        # Removed chunking parameters - now handled by PDF processor
    
    @property
    def worker_type(self) -> str:
        return "processor"
    
    async def get_next_task(self) -> Optional[ProcessTask]:
        """Get next process task from queue."""
        return await self.queue_manager.get_process_task(timeout=1.0)
    
    async def process_task(self, task: ProcessTask) -> bool:
        """
        Process a page: route to PDF processor for Docling.
        
        SIMPLIFIED ARCHITECTURE:
        - All crawled pages have PDFs (generated by Playwright)
        - All pages are routed to PDF processor for Docling + HybridChunker
        - No more DOM text extraction or OCR decision logic here
        
        Args:
            task: ProcessTask with page to process
            
        Returns:
            True if successful
        """
        # Defensive check: ensure task is correct type
        if not isinstance(task, ProcessTask):
            logger.error(
                f"[{self.worker_id}] Received wrong task type: {type(task).__name__}. "
                f"Expected ProcessTask. Skipping task."
            )
            return False
        
        task.mark_started(self.worker_id)
        
        page = task.page
        logger.info(f"[{self.worker_id}] Routing page to PDF processor: {page.url}")
        
        try:
            # Verify PDF exists (should always be true for crawled pages)
            if not page.pdf_path or not page.pdf_path.exists():
                logger.error(
                    f"[{self.worker_id}] Page missing PDF (this shouldn't happen): {page.url}. "
                    "Crawler should always generate PDFs."
                )
                task.mark_failed("Missing PDF file")
                return False
            
            # Check PDF size - skip processing for excessively large files
            pdf_size_mb = page.pdf_path.stat().st_size / (1024 * 1024)
            
            from app.config import PDF_MAX_PROCESSING_SIZE_MB
            
            if PDF_MAX_PROCESSING_SIZE_MB > 0 and pdf_size_mb > PDF_MAX_PROCESSING_SIZE_MB:
                # Large PDF - save to separate storage and skip Docling processing
                logger.warning(
                    f"[{self.worker_id}] Large PDF detected ({pdf_size_mb:.2f} MB > {PDF_MAX_PROCESSING_SIZE_MB} MB): {page.url}. "
                    "Saving to storage only, skipping Docling processing."
                )
                
                await self._save_large_pdf_to_storage(page, task.website_url, pdf_size_mb)
                task.mark_completed(needs_ocr=False, chunks=0)
                
                return True
            
            # Route ALL pages to PDF processor for Docling + HybridChunker
            logger.info(
                f"[{self.worker_id}] Routing to PDF processor for Docling "
                f"({pdf_size_mb:.2f} MB): {page.url}"
            )
            
            pdf_task = PdfTask(
                task_id=f"{task.task_id}_pdf",
                page=page,
                website_url=task.website_url,
                crawl_session_id=task.crawl_session_id,
                priority=task.priority,
            )
            
            await self.queue_manager.put_pdf_task(pdf_task)
            task.mark_completed(needs_ocr=False, chunks=0)
            
            logger.debug(f"[{self.worker_id}] Successfully routed to PDF queue: {page.url}")
            return True
            
        except Exception as e:
            logger.error(
                f"[{self.worker_id}] Failed to route task for {page.url}: {e}",
                exc_info=True
            )
            
            # Move to dead letter queue if max retries exceeded
            if task.retry_count >= task.max_retries:
                await self.queue_manager.put_dead_letter(
                    task,
                    f"Routing failed: {e}"
                )
            else:
                # Requeue for retry
                task.mark_failed(str(e))
                await self.queue_manager.put_process_task(task)
                logger.debug(
                    f"[{self.worker_id}] Requeued task "
                    f"(retry {task.retry_count}/{task.max_retries})"
                )
            
            return False
    
    async def _save_large_pdf_to_storage(
        self,
        page: Page,
        website_url: str,
        pdf_size_mb: float,
    ):
        """
        Save large PDF to separate storage folder and MongoDB without processing.
        
        Args:
            page: Page with large PDF
            website_url: Website URL for grouping
            pdf_size_mb: Size of PDF in MB
        """
        try:
            from app.config import LARGE_PDF_STORAGE_DIR
            from app.services.document_store import DocumentStore
            from app.schemas.document import DocumentStatus
            import shutil
            import uuid
            
            # Create large PDF storage directory
            storage_dir = Path(LARGE_PDF_STORAGE_DIR)
            storage_dir.mkdir(parents=True, exist_ok=True)
            
            # Copy PDF to large files storage
            dest_path = storage_dir / page.pdf_path.name
            shutil.copy2(page.pdf_path, dest_path)
            
            # Save metadata to MongoDB
            store = DocumentStore.from_config()
            
            # Extract original filename from URL
            original_filename = page.url.split("/")[-1] or "document.pdf"
            
            await asyncio.to_thread(
                store.create_document,
                original_file=original_filename,
                source_url=page.url,
                file_path=str(dest_path),
                crawl_session_id=str(uuid.uuid4()),  # Could use crawl_session_id from task
                file_size=int(pdf_size_mb * 1024 * 1024),
                crawl_depth=page.depth if hasattr(page, 'depth') else 0,
                status=DocumentStatus.STORED,  # Mark as stored, not processed
            )
            
            logger.info(
                f"[{self.worker_id}] Saved large PDF to storage: {dest_path} "
                f"({pdf_size_mb:.2f} MB) - URL: {page.url}"
            )
            
        except Exception as e:
            logger.error(
                f"[{self.worker_id}] Failed to save large PDF to storage: {e}",
                exc_info=True
            )